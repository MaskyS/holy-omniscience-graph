- Extracting context-independent information
    - Categorizing the sharing of ideas, and the sharing of ideas about ideas (meta-information). The latter is heavily context-dependent, and often of ephemereal nature. [Chatting With Glue](<Chatting With Glue.md>) expands on this.
    - [ ] Expand on this analogy- writing code to do one thing, vs realizing a pattern and building functions, which eventually leads to a library of functions (truths/concepts)
- Deriving implicit meaning
- [ ] Harmonizing a million voices
    - How do we know that a piece of information is the best? How can 
multiple people employ their skills in tandem with each other to improve
 information?
    - Let's take a look at open source software. There, collaborative improvement occurs through pull-requests, where people can freely use/fork/remix code from a single source of information (the source repository code).
    - Then, to make improvements, the code is edited at various parts— changing one statement, one function, one character at a time— and then we create [pull-requests](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/about-pull-requests) to merge the changes into the main repo.
    - But what about improving English texts? The equivalent would be Wikipedia, 
but as (edit) history has demonstrated, this is not efficient— you usually can't just edit one word at a time, and there's a million ways to say the same thing. What now?
    - Well, what if we improved the written word by getting a maximum of people to interpret a subject, and then used AI to summarize their answers into a single entry? In other words, we let AI find the meme— an automated version of [The Flip Side](https://www.theflipside.io/faq).
    - Now imagine an AI-boosted long-form version of this, applied on, say, book reviews. Or even better, book notes. Or on lecture notes produced by students, for the next batch of students (a tweet I read also proposed that the teacher could use this to learn which parts of their lesson were memorable (or memeable) and which parts need reworking).
    - While this principle is applicable to virtually any subject, it comes with a pitfall: [the wisdom of the crowds isn't always right](https://www.santafe.edu/news-center/news/new-study-improves-crowd-wisdom-estimates), but I touch on addressing this later on.
- [ ] Notes from og article (to rewrite)
    - With GPT-n, information distillation is vastly augmented.
    - Within a few years, business books might become obsolete because GPT-n cuts out all the anecdotal fluff in a business book, and summarizes the main 
topics for the reader beforehand.
    - GPT-n can enable a better, faster process for picking out our best, timeless works from years of consistent practice— for e.g., your best blog posts in the last 10 years automatically get compressed into a book.
    - Another application is summarizing long-form content like a [Youtube](<Youtube.md>) lecture (both through the text transcript and video content analysis), which you could read to decide if it's worth watching the lecture— or maybe just drop the lecture altogether and use the generated notes.
    - There aren't enough hours in the day to listen to podcasts— what if we could have automated [podcast summaries](https://podcastnotes.org/category/podcast-notes-summaries/)? The list goes on.
- Translate slow (and unsearchable) mediums to semantic text
    - Auto summarize [podcasts](<podcasts.md>) and videos
    - Show overview of topics and principles covered
    - "https://twitter.com/nattyover/status/1282674617623154689 #[Information distillation](<Information distillation.md>) [podcasts](<podcasts.md>) #[Unstructured Information](<Unstructured Information.md>) "
- Show content overlap between similar resources

# Backlinks
## [Democratize curation](<Democratize curation.md>)
- Curation is largely ephemereal and one-dimensional due to a lack of [Information distillation](<Information distillation.md>)

## [Information distillation](<Information distillation.md>)
- "https://twitter.com/nattyover/status/1282674617623154689 #[Information distillation](<Information distillation.md>)

## [January 10th, 2021](<January 10th, 2021.md>)
- https://twitter.com/azlenelza/status/1269696510611959808 #[Azlen Elza](<Azlen Elza.md>) #[Max Kriegers](<Max Kriegers.md>) #[Chatting With Glue](<Chatting With Glue.md>) #[Information distillation](<Information distillation.md>)

## [January 8th, 2021](<January 8th, 2021.md>)
- https://twitter.com/nattyover/status/1282674617623154689 #[Information distillation](<Information distillation.md>)

## [The Information Platform of The Future](<The Information Platform of The Future.md>)
- {{embed: [Information distillation](<Information distillation.md>)}

